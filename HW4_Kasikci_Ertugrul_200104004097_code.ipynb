{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**CSE455 HW3 Ertugrul Kasikci 200104004097**"
      ],
      "metadata": {
        "id": "gKlDn2XnS3EJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import Perceptron\n",
        "import numpy as np\n",
        "  \n",
        "\n",
        "df = datasets.load_breast_cancer()\n",
        "X = df.data\n",
        "y = df.target\n",
        "\n",
        "print(type(df))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to the training data\n",
        "scaler.fit(X_train)\n",
        "\n",
        "# Transform the training data\n",
        "X_train = scaler.transform(X_train)\n",
        "\n",
        "# Transform the test data\n",
        "X_test = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbTEbH_kaEBr",
        "outputId": "695c4cd8-15fb-4596-b6fe-d9a67ddfe424"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.utils._bunch.Bunch'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part I: Select a dataset:**\n",
        "\n",
        "I used datasets.load_breast_cancer() statement to get the breast cancer dataset, then divided it into X_train, X_test, y_train, y_test using the train_test_split function.\n",
        "\n",
        "To get higher accuracy results, I used StandartScaler class so that all the attributes of the data will have similar effects on the result."
      ],
      "metadata": {
        "id": "vRV3wEXqSnSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP with one hidden layer\n",
        "mlp1 = MLPClassifier(hidden_layer_sizes=(7,), early_stopping=True, random_state=42)\n",
        "scores = cross_val_score(mlp1, X_train, y_train, cv=5)\n",
        "print(\"Average cross-validation score of MLP with 1 hidden layer: \", scores.mean())\n",
        "\n",
        "# MLP with two hidden layers\n",
        "mlp2 = MLPClassifier(hidden_layer_sizes=(5, 10), early_stopping=True, random_state=42)\n",
        "scores = cross_val_score(mlp2, X_train, y_train, cv=5)\n",
        "print(\"Average cross-validation score of MLP with 2 hidden layers: \", scores.mean())\n",
        "\n",
        "# MLP with three hidden layers\n",
        "mlp3 = MLPClassifier(hidden_layer_sizes=(4, 6, 4), early_stopping=True, random_state=42)\n",
        "scores = cross_val_score(mlp3, X_train, y_train, cv=5)\n",
        "print(\"Average cross-validation score of MLP with 3 hidden layers: \", scores.mean())\n",
        "\n",
        "# MLP with four hidden layers\n",
        "mlp4 = MLPClassifier(hidden_layer_sizes=(5, 16, 9, 11), early_stopping=True,random_state=42)\n",
        "scores = cross_val_score(mlp4, X_train, y_train, cv=5)\n",
        "print(\"Average cross-validation score of MLP with 4 hidden layers: \", scores.mean())\n",
        "\n",
        "# MLP with five hidden layers\n",
        "mlp5 = MLPClassifier(hidden_layer_sizes=(6, 6, 10, 10, 13), early_stopping=True,random_state=42)\n",
        "scores = cross_val_score(mlp5, X_train, y_train, cv=5)\n",
        "print(\"Average cross-validation score of MLP with 5 hidden layers: \", scores.mean())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EF3x8dO5SncS",
        "outputId": "9df429fe-c962-4128-9ba1-6de2367e208a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average cross-validation score of MLP with 1 hidden layer:  0.8813186813186812\n",
            "Average cross-validation score of MLP with 2 hidden layers:  0.8791208791208792\n",
            "Average cross-validation score of MLP with 3 hidden layers:  0.6285714285714286\n",
            "Average cross-validation score of MLP with 4 hidden layers:  0.6285714285714286\n",
            "Average cross-validation score of MLP with 5 hidden layers:  0.7736263736263737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part II: Train a multi-layer perceptron:**\n",
        "\n",
        "I trained 5 different MLPs with different numbers of layers and perceptrons. I used the early_stopping attribute to prevent overfitting.\n",
        "\n",
        "To get their accuracy, I used cross-validation with cv = 5."
      ],
      "metadata": {
        "id": "KWj0-OmsSnng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class customMLPClassifer(MLPClassifier):\n",
        "    def resample_with_replacement(self, X_train, y_train, sample_weight):\n",
        "\n",
        "        # normalize sample_weights if not already\n",
        "        sample_weight = sample_weight / sample_weight.sum(dtype=np.float64)\n",
        "\n",
        "        X_train_resampled = np.zeros((len(X_train), len(X_train[0])), dtype=np.float32)\n",
        "        y_train_resampled = np.zeros((len(y_train)), dtype=int)\n",
        "        for i in range(len(X_train)):\n",
        "            # draw a number from 0 to len(X_train)-1\n",
        "            draw = np.random.choice(np.arange(len(X_train)), p=sample_weight)\n",
        "\n",
        "            # place the X and y at the drawn number into the resampled X and y\n",
        "            X_train_resampled[i] = X_train[draw]\n",
        "            y_train_resampled[i] = y_train[draw]\n",
        "\n",
        "        return X_train_resampled, y_train_resampled\n",
        "\n",
        "    def fit(self, X, y, sample_weight=None):\n",
        "        if sample_weight is not None:\n",
        "            X, y = self.resample_with_replacement(X, y, sample_weight)\n",
        "\n",
        "        return self._fit(X, y, incremental=(self.warm_start and hasattr(self, \"classes_\")))\n",
        "\n",
        "# This is a custom implementation of MLP which makes us able to use it as a estimator in AdaBoostClassifier.\n",
        "mlp1 = customMLPClassifer(hidden_layer_sizes=(7,), early_stopping=True, random_state=42)\n",
        "scores = cross_val_score(mlp1, X_train, y_train, cv=5)\n",
        "print(\"Average cross-validation score of custom MLP with 1 hidden layer: \", scores.mean())\n",
        "\n",
        "\n",
        "# Initialize the AdaBoost classifier\n",
        "ada_clf = AdaBoostClassifier(estimator=mlp1, n_estimators=50, random_state=42)\n",
        "\n",
        "# Fit the AdaBoost classifier to the training data\n",
        "ada_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict the test data\n",
        "y_pred = ada_clf.predict(X_test)\n",
        "\n",
        "# Compute the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy of AdaBoost classifier with one-hidden-layer perceptron as the base classifier: \", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHyZD566Sz-C",
        "outputId": "15749c8c-8c79-42cf-adfb-67d11aef1247"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average cross-validation score of custom MLP with 1 hidden layer:  0.8813186813186812\n",
            "Accuracy of AdaBoost classifier with one-hidden-layer perceptron as the base classifier:  0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part III: Train a multi-layer perceptron:**\n",
        "\n",
        "To be able to use one hidden layer MLP as the base classifier of the AdaBoost classifier, I needed to implement it myself because the sample_weight attribute which is used when creating the AdaBoost classifier normally uses a decision tree. When I try to pass my one hidden layer MLP it gave me an error:\n",
        "\n",
        "ValueError: NoneType doesn't support sample_weight.\n",
        "\n",
        "customMLPClassifer solves this issue.\n",
        "\n",
        "**customMLPClassifer:**\n",
        "\n",
        "This custom MLP classifier has been overridden with a new fit method that takes sample_weight as an argument. If sample_weight is provided, the method will resample the training data according to these weights before fitting the model. This allows the MLP classifier to function as a weak learner in the AdaBoost algorithm\n",
        "\n",
        "The accuracy of the AdaBoost learner us quite satisfying."
      ],
      "metadata": {
        "id": "x_4wVMAoX448"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "''' # Create a Perceptron object\n",
        "perceptron = Perceptron()\n",
        "\n",
        "# Create a Random Forest Classifier object with the Perceptron as the base estimator\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, base_estimator=perceptron)\n",
        "\n",
        "# Fit the Random Forest Classifier to the training data\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict the test data\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "\n",
        "# Compute the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy of Random Forest Classifier with Perceptron base estimator: \", accuracy) '''"
      ],
      "metadata": {
        "id": "PplVmEHNX5Em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part IV: Train a random decision forest where each decision in the forest is in turn is a trainable perceptron:**\n",
        "\n",
        "As far as I understand to be able to implement this I need to make my own random forest and use perceptron in the decision trees instead of decisions. Because there is no way to change the base_estimator of random forest. Decision trees are deeply embedeed in them. I think this is quite hard thing to do and there is no enough time to do it."
      ],
      "metadata": {
        "id": "DDSjG4hMCVLt"
      }
    }
  ]
}